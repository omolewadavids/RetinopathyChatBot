{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetic Retinopathy Detection ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"retQA.json\") as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the word Vocabulary and Tokenizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "labels = []\n",
    "responses = []\n",
    "docs_x = []\n",
    "docs_y = []\n",
    "for intent in data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        wrds = nltk.word_tokenize(pattern)\n",
    "        words.extend([w for w in wrds if not w in stop_words])\n",
    "        docs_x.append(wrds)\n",
    "        docs_y.append(intent[\"tag\"])\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for i in range(len(docs_x)):\n",
    "    filtered_sentence = [w for w in docs_x[i] if not w in stop_words]\n",
    "    sentences.append(filtered_sentence)\n",
    "        \n",
    "for intent in data[\"intents\"]:\n",
    "    for response in intent[\"responses\"]:\n",
    "        resp = nltk.word_tokenize(response)\n",
    "        responses.extend([w for w in resp if not w in stop_words])\n",
    "\n",
    "words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
    "responses = [stemmer.stem(r.lower()) for r in responses if r != \"?\"]\n",
    "vocab = sorted(set(words + responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vectorizing the word sequences.....')\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word representation model using FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "#from gensim.models.fasttext import FastText\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import WordPunctTokenizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(document):\n",
    "        # Remove all the special characters\n",
    "        document = re.sub(r'\\W', ' ', str(document))\n",
    "\n",
    "        # remove all single characters\n",
    "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "        # Remove single characters from the start\n",
    "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "        # Removing prefixed 'b'\n",
    "        document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        document = document.lower()\n",
    "\n",
    "        # Lemmatization\n",
    "        tokens = document.split()\n",
    "        tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "        tokens = [word for word in tokens if word not in en_stop]\n",
    "        tokens = [word for word in tokens if len(word) > 3]\n",
    "\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "\n",
    "        return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_text('what are the symptoms of diabetic retinopathy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model10 = FastText(size=15, window=3, min_count=5)\n",
    "model10.build_vocab(sentences=word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 15\n",
    "window_size = 3\n",
    "min_word = 5\n",
    "down_sampling = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fast_text_model = FastText('corpus.txt',\n",
    "                      size=embedding_size,\n",
    "                      window=window_size,\n",
    "                      min_count=min_word,\n",
    "                      sample=down_sampling,\n",
    "                      sg=1,\n",
    "                      iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fasttext import train_unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = train_unsupervised(input='corpus.txt', epoch=2, lr=1.0, wordNgrams=3, verbose=2, minCount=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr.ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "# Skipgram model :\n",
    "skip_model = fasttext.train_unsupervised('corpus.txt', model='skipgram')\n",
    "\n",
    "# or, cbow model :\n",
    "cbow_model = fasttext.train_unsupervised('corpus.txt', model='cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(skip_model.words)   # list of words in dictionary\n",
    "print(cbow_model.words)   # list of words in dictionary\n",
    "print(skip_model['excellent']) # get the vector of the word 'retinopathy'\n",
    "print(cbow_model['retinopathy']) # get the vector of the word 'retinopathy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(data):\n",
    "    inputs = []\n",
    "    wrds = nltk.word_tokenize(data)\n",
    "    wrds = [w for w in wrds if not w in stop_words]\n",
    "    words = [stemmer.stem(w.lower()) for w in wrds if w != \"?\"]\n",
    "    # Remove words not in vocab\n",
    "    removed_words = [words.pop(i) for i, w in enumerate(words) if w not in vocab]\n",
    "    \n",
    "    for w in removed_words:\n",
    "        inputs.append(word_idx[w])\n",
    "    return inputs\n",
    "    #return pad_sequences(inputs, maxlen = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for ls in sentences:\n",
    "    sent = [stemmer.stem(w.lower()) for w in ls if w != \"?\"]\n",
    "    inner_list = []\n",
    "    for ent in sent:\n",
    "        inner_list.append(word_idx[ent])\n",
    "    corpus.append(inner_list)\n",
    "X_train = pad_sequences(corpus, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(docs_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing the Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tags dictionary\n",
    "y = {}\n",
    "y = dict((c, i) for i, c in enumerate(set(docs_y))) \n",
    "\n",
    "# Convert tags to interger representation\n",
    "y_train = []\n",
    "for entry in docs_y:\n",
    "    y_train.append(y[entry])\n",
    "\n",
    "# Convert to categorical using keras module\n",
    "import keras\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, \n",
    "                                                    test_size=.3,\n",
    "                                                   random_state=1,\n",
    "                                                   stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 8\n",
    "\n",
    "model_1 = Sequential()\n",
    "model_1.add(Embedding(len(vocab)+1, 8, input_length=maxlen))\n",
    "model_1.add(layers.Bidirectional(layers.LSTM(maxlen)))\n",
    "model_1.add(Dense(12, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_1.fit(X_train, y_train, epochs=500, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "save_path ='/Users/adara/CAPSTONEPROJECT/eyeNetBot'\n",
    "pickle.dump(vocab, open(os.path.join(save_path, \"vocab.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model, if it exits, load vocab too\n",
    "from keras.models import load_model\n",
    "save_path = '/Users/adara/CAPSTONEPROJECT/eyeNetBot'\n",
    "model_1 = load_model(os.path.join(save_path, \"chatModel.h5\"))\n",
    "#vocab = pickle.load(open(os.path.join(save_path, \"vocab.pkl\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving and loading a model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"skip_model.bin\")\n",
    "model.save_model(\"cbow_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_model = fasttext.load_model(\"skip_model.bin\")\n",
    "cbow_model = fasttext.load_model(\"cbow_model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(data):\n",
    "    inputs = []\n",
    "    wrds = nltk.word_tokenize(data)\n",
    "    words = [stemmer.stem(w.lower()) for w in wrds if w != \"?\"]\n",
    "    filtered_sentence = [w for w in words if not w in stop_words]\n",
    "    [filtered_sentence.pop(i) for i, w in enumerate(filtered_sentence) if w not in vocab]\n",
    "    \n",
    "    inner_list = []\n",
    "    \n",
    "    for ent in filtered_sentence:\n",
    "        inner_list.append(word_idx[ent])\n",
    "    inputs.append(inner_list)\n",
    "    \n",
    "    return pad_sequences(inputs, maxlen = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tags dictionary\n",
    "labels = {}\n",
    "labels = dict((i, c) for i, c in enumerate(set(docs_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = vectorize('what causes the disease')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_1.predict(n, 12)[0]\n",
    "results_index = np.argmax(results)\n",
    "print(labels[results_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    " \n",
    "from sqlite3 import Error\n",
    " \n",
    "def sql_connection():\n",
    " \n",
    "    try:\n",
    " \n",
    "        con = sqlite3.connect('retQA.db')\n",
    " \n",
    "        return con\n",
    " \n",
    "    except Error:\n",
    " \n",
    "        print(Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_table(con):\n",
    " \n",
    "    cursorObj = con.cursor()\n",
    " \n",
    "    cursorObj.execute(\"CREATE TABLE QUESTIONS(Question text)\")\n",
    " \n",
    "    con.commit()\n",
    "\n",
    "con = sql_connection()\n",
    " \n",
    "sql_table(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gtts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "import playsound\n",
    "import speech_recognition as sr\n",
    "from gtts import gTTS\n",
    "from pathlib import Path\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    print(\"start talking with the bot (type quit to stop)!\")\n",
    "    num = 100\n",
    "    while True:\n",
    "                \n",
    "        inp = input(\"You: \")\n",
    "        num = num + 1 \n",
    "        \n",
    "        if inp.lower() == \"quit\":\n",
    "            tts = gTTS(text = 'bye', lang = \"en\", slow=False)\n",
    "            filename = 'chat{}.mp3'.format(num)\n",
    "            tts.save(filename)\n",
    "            playsound.playsound(filename)\n",
    "            break\n",
    "        inp = vectorize(inp)\n",
    "        results = model_1.predict(inp, 12)[0]\n",
    "        results_index = np.argmax(results)\n",
    "        output = labels[results_index]\n",
    "        \n",
    "        if results[results_index] > 0.5:\n",
    "            for tg in data[\"intents\"]:\n",
    "                if tg['tag'] == output:\n",
    "                    responses = tg['responses']\n",
    "            tts = gTTS(text = random.choice(responses), lang = \"en\", slow=False)\n",
    "            filename = 'chat{}.mp3'.format(num)\n",
    "            tts.save(filename)\n",
    "            playsound.playsound(filename)\n",
    "        else:\n",
    "            tts = gTTS(text = \"I didn't understand that, try again.\", lang = \"en\", slow=False)\n",
    "            filename = 'chat{}.mp3'.format(num)\n",
    "            tts.save(filename)\n",
    "            playsound.playsound(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = 'omolewa'\n",
    "con = sqlite3.connect('retQA.db')\n",
    "cursor = con.cursor()\n",
    "cursor.execute(\"\"\"INSERT INTO QUESTIONS (question) VALUES (?)\"\"\", [(inp)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"SELECT * FROM QUESTIONS\"\"\")\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anvil.server\n",
    "anvil.server.connect(\"here comes your anvil link code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anvil.server\n",
    "\n",
    "anvil.server.connect(\"35KHU4CLLVXFT6LWX4G34K5F-5RJCZTHEBWY35MGS\")\n",
    "@anvil.server.callable\n",
    "def botChat(inp):\n",
    "    print(\"Making botchat call\")\n",
    "    if inp.lower() == \"quit\":\n",
    "        return 'bye'\n",
    "    \n",
    "    inp = vectorize(inp)\n",
    "    results = model_1.predict(inp, 12)[0]\n",
    "    results_index = np.argmax(results)\n",
    "    output = labels[results_index]\n",
    "    \n",
    "    if results[results_index] > 0.5:\n",
    "        for tg in data[\"intents\"]:\n",
    "            if tg['tag'] == output:\n",
    "                print(\"Found match\")\n",
    "                print(tg['tag'])\n",
    "                responses = tg['responses']\n",
    "            \n",
    "        if responses:\n",
    "            for response in responses:\n",
    "                speak_text(reponse)\n",
    "        return random.choice(responses)\n",
    "    else:\n",
    "        # create a database and store the question\n",
    "        return \"I didn't understand that, try again.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "labels = []\n",
    "responses = []\n",
    "x = []\n",
    "y = []\n",
    "for intent in data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        sent = preprocess_text(pattern)\n",
    "        x.append(sent)\n",
    "        y.append(intent[\"tag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = []\n",
    "for i in range(len(x)):\n",
    "    kk = nltk.word_tokenize(x[i])\n",
    "    for j in range(len(kk)):\n",
    "        ff.append(kk[j])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg = []\n",
    "for intent in data[\"intents\"]:\n",
    "    for response in intent[\"responses\"]:\n",
    "        resp = preprocess_text(response)\n",
    "        a = [w for w in nltk.word_tokenize(resp)]\n",
    "        for i in range(len(a)):\n",
    "            gg.append(a[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.word_tokenize(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = ['what', 'is', 'afsdguyg', 'diabetic', 'retinopathy', 'bukola']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sent.pop(i) for i, w in enumerate(sent) if w not in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww = []\n",
    "for w in sent:\n",
    "    if w in vocab:\n",
    "        ww.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in sent:\n",
    "    if w not in vocab:\n",
    "        print(enumerate(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
